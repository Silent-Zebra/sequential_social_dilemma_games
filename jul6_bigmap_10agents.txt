/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-07-06_22-08-42_13370/logs.
Waiting for redis server at 127.0.0.1:63118 to respond...
Waiting for redis server at 127.0.0.1:62798 to respond...
Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.
Starting the Plasma object store with 20.0 GB memory using /dev/shm.

======================================================================
View the web UI at http://localhost:8889/notebooks/ray_ui.ipynb?token=a6aeb2cc4369dd9f77b251fcaaf985dad034df768c294295
======================================================================

== Status ==
Using FIFO scheduling algorithm.
Resources requested: 0/2 CPUs, 0/1 GPUs
Memory usage on this node: 7.9/202.5 GB

Created LogSyncer for /h/zhaostep/ray_results/harvest_A3C/A3C_harvest_env_0_2020-07-06_22-08-43y7do0cos -> 
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 7.9/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING

/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-07-06 22:08:55,683	INFO policy_evaluator.py:262 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)
2020-07-06 22:08:55.684406: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
From /h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/rllib/models/lstm.py:59: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-07-06 22:09:07,930	INFO policy_evaluator.py:262 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
2020-07-06 22:09:07,930	INFO policy_evaluator.py:262 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)
2020-07-06 22:09:07.931463: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-07-06 22:09:07.932203: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
From /h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/rllib/models/lstm.py:59: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').
From /h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/rllib/models/lstm.py:59: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.
Instructions for updating:
This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-09-28
  done: false
  episode_len_mean: .nan
  episode_reward_max: .nan
  episode_reward_mean: .nan
  episode_reward_min: .nan
  episodes_this_iter: 0
  episodes_total: 0
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 4.596
    dispatch_time_ms: 13.1
    learner:
      cur_lr: 0.0013599999947473407
      grad_gnorm: 40.000003814697266
      policy_entropy: 164.960693359375
      policy_loss: 40.92182540893555
      var_gnorm: 18.078317642211914
      vf_explained_var: 0.03237229585647583
      vf_loss: 18.347183227539062
    num_steps_sampled: 10000
    num_steps_trained: 10000
    wait_time_ms: 170.227
  iterations_since_restore: 1
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 31.122663259506226
  time_this_iter_s: 31.122663259506226
  time_total_s: 31.122663259506226
  timestamp: 1594087768
  timesteps_since_restore: 10000
  timesteps_this_iter: 10000
  timesteps_total: 10000
  training_iteration: 1
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 9.7/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 31 s, 1 iter, 10000 ts, nan rew

[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
agent-1: 15.0
agent-2: -71.0
agent-3: -134.0
agent-4: -82.0
agent-5: -182.0
agent-6: -132.0
agent-7: 81.0
agent-8: -103.0
agent-9: 67.0
agent-10: -33.0
Sum Reward: -574.0
Avg Reward: -57.4
Min Reward: -182.0
Max Reward: 81.0
Gini Coefficient: -0.8240418118466899
20:20 Ratio: -0.46835443037974683
Max-min Ratio: -0.44505494505494503
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-09-46
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -574.0
  episode_reward_mean: -574.0
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 1
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.836
    dispatch_time_ms: 6.827
    learner:
      cur_lr: 0.0013593339826911688
      grad_gnorm: 40.0
      policy_entropy: 126.06654357910156
      policy_loss: 43.37526321411133
      var_gnorm: 18.219898223876953
      vf_explained_var: 0.269353449344635
      vf_loss: 49.256412506103516
    num_steps_sampled: 20000
    num_steps_trained: 20000
    wait_time_ms: 181.125
  iterations_since_restore: 2
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 49.62368178367615
  time_this_iter_s: 18.501018524169922
  time_total_s: 49.62368178367615
  timestamp: 1594087786
  timesteps_since_restore: 20000
  timesteps_this_iter: 10000
  timesteps_total: 20000
  training_iteration: 2
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 10.2/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 49 s, 2 iter, 20000 ts, -574 rew

[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.[0m
agent-1: -77.0
agent-2: 2.0
agent-3: 46.0
agent-4: -197.0
agent-5: -155.0
agent-6: 8.0
agent-7: -29.0
agent-8: 23.0
agent-9: 66.0
agent-10: 92.0
Sum Reward: -221.0
Avg Reward: -22.1
Min Reward: -197.0
Max Reward: 92.0
Gini Coefficient: -2.228506787330317
20:20 Ratio: -0.44886363636363635
Max-min Ratio: -0.467005076142132
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-10-04
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: -221.0
  episode_reward_mean: -397.5
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 2
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 3.873
    dispatch_time_ms: 6.767
    learner:
      cur_lr: 0.0013586679706349969
      grad_gnorm: 40.00000762939453
      policy_entropy: 116.03890991210938
      policy_loss: 35.82439041137695
      var_gnorm: 18.547624588012695
      vf_explained_var: 0.08281415700912476
      vf_loss: 66.59579467773438
    num_steps_sampled: 30000
    num_steps_trained: 30000
    wait_time_ms: 195.822
  iterations_since_restore: 3
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 67.84619355201721
  time_this_iter_s: 18.222511768341064
  time_total_s: 67.84619355201721
  timestamp: 1594087804
  timesteps_since_restore: 30000
  timesteps_this_iter: 10000
  timesteps_total: 30000
  training_iteration: 3
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 10.7/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 67 s, 3 iter, 30000 ts, -398 rew

agent-1: 182.0
agent-2: 166.0
agent-3: 163.0
agent-4: 181.0
agent-5: 154.0
agent-6: 175.0
agent-7: 143.0
agent-8: 167.0
agent-9: 186.0
agent-10: 151.0
Sum Reward: 1668.0
Avg Reward: 166.8
Min Reward: 143.0
Max Reward: 186.0
Gini Coefficient: 0.04652278177458034
20:20 Ratio: 1.251700680272109
Max-min Ratio: 1.3006993006993006
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-10-25
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 291.0
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 3
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.729
    dispatch_time_ms: 7.052
    learner:
      cur_lr: 0.001358001958578825
      grad_gnorm: 39.999996185302734
      policy_entropy: 95.73690032958984
      policy_loss: -52.136627197265625
      var_gnorm: 18.850576400756836
      vf_explained_var: 0.5580810308456421
      vf_loss: 93.3768081665039
    num_steps_sampled: 40000
    num_steps_trained: 40000
    wait_time_ms: 196.654
  iterations_since_restore: 4
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 88.4940083026886
  time_this_iter_s: 20.647814750671387
  time_total_s: 88.4940083026886
  timestamp: 1594087825
  timesteps_since_restore: 40000
  timesteps_this_iter: 10000
  timesteps_total: 40000
  training_iteration: 4
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 11.3/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 88 s, 4 iter, 40000 ts, 291 rew

agent-1: 163.0
agent-2: 147.0
agent-3: 140.0
agent-4: 145.0
agent-5: 23.0
agent-6: 144.0
agent-7: 104.0
agent-8: 144.0
agent-9: 163.0
agent-10: 95.0
Sum Reward: 1268.0
Avg Reward: 126.8
Min Reward: 23.0
Max Reward: 163.0
Gini Coefficient: 0.1550473186119874
20:20 Ratio: 2.76271186440678
Max-min Ratio: 7.086956521739131
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-10-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 535.25
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 4
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.482
    dispatch_time_ms: 5.964
    learner:
      cur_lr: 0.001357335946522653
      grad_gnorm: 40.0
      policy_entropy: 87.04920959472656
      policy_loss: 46.608890533447266
      var_gnorm: 19.186717987060547
      vf_explained_var: 0.6219722032546997
      vf_loss: 81.15579223632812
    num_steps_sampled: 50000
    num_steps_trained: 50000
    wait_time_ms: 201.896
  iterations_since_restore: 5
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 108.13243293762207
  time_this_iter_s: 19.63842463493347
  time_total_s: 108.13243293762207
  timestamp: 1594087845
  timesteps_since_restore: 50000
  timesteps_this_iter: 10000
  timesteps_total: 50000
  training_iteration: 5
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 11.8/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 108 s, 5 iter, 50000 ts, 535 rew

agent-1: 92.0
agent-2: 165.0
agent-3: 139.0
agent-4: 105.0
agent-5: 158.0
agent-6: 175.0
agent-7: 98.0
agent-8: 87.0
agent-9: 182.0
agent-10: 135.0
Sum Reward: 1336.0
Avg Reward: 133.6
Min Reward: 87.0
Max Reward: 182.0
Gini Coefficient: 0.14476047904191616
20:20 Ratio: 1.994413407821229
Max-min Ratio: 2.0919540229885056
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-11-07
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 695.4
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 5
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.654
    dispatch_time_ms: 5.801
    learner:
      cur_lr: 0.001356670050881803
      grad_gnorm: 40.0000114440918
      policy_entropy: 83.75164794921875
      policy_loss: 99.61499786376953
      var_gnorm: 19.78830909729004
      vf_explained_var: 0.43219393491744995
      vf_loss: 260.8778076171875
    num_steps_sampled: 60000
    num_steps_trained: 60000
    wait_time_ms: 201.813
  iterations_since_restore: 6
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 130.10894131660461
  time_this_iter_s: 21.976508378982544
  time_total_s: 130.10894131660461
  timestamp: 1594087867
  timesteps_since_restore: 60000
  timesteps_this_iter: 10000
  timesteps_total: 60000
  training_iteration: 6
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 12.3/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 130 s, 6 iter, 60000 ts, 695 rew

agent-1: 126.0
agent-2: 110.0
agent-3: 180.0
agent-4: 194.0
agent-5: 111.0
agent-6: 140.0
agent-7: 151.0
agent-8: 99.0
agent-9: 155.0
agent-10: 158.0
Sum Reward: 1424.0
Avg Reward: 142.4
Min Reward: 99.0
Max Reward: 194.0
Gini Coefficient: 0.1178370786516854
20:20 Ratio: 1.7894736842105263
Max-min Ratio: 1.9595959595959596
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-11-26
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 816.8333333333334
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 6
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.53
    dispatch_time_ms: 5.507
    learner:
      cur_lr: 0.0013560040388256311
      grad_gnorm: 40.000030517578125
      policy_entropy: 80.95339965820312
      policy_loss: -35.3884391784668
      var_gnorm: 20.15573501586914
      vf_explained_var: 0.3932427167892456
      vf_loss: 84.64932250976562
    num_steps_sampled: 70000
    num_steps_trained: 70000
    wait_time_ms: 206.185
  iterations_since_restore: 7
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 149.6692295074463
  time_this_iter_s: 19.560288190841675
  time_total_s: 149.6692295074463
  timestamp: 1594087886
  timesteps_since_restore: 70000
  timesteps_this_iter: 10000
  timesteps_total: 70000
  training_iteration: 7
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 12.9/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 149 s, 7 iter, 70000 ts, 817 rew

agent-1: 174.0
agent-2: 90.0
agent-3: 132.0
agent-4: 167.0
agent-5: 106.0
agent-6: 167.0
agent-7: 127.0
agent-8: 134.0
agent-9: 147.0
agent-10: 178.0
Sum Reward: 1422.0
Avg Reward: 142.2
Min Reward: 90.0
Max Reward: 178.0
Gini Coefficient: 0.11153305203938116
20:20 Ratio: 1.7959183673469388
Max-min Ratio: 1.9777777777777779
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-11-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 903.2857142857143
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 7
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.663
    dispatch_time_ms: 6.039
    learner:
      cur_lr: 0.0013553380267694592
      grad_gnorm: 26.463441848754883
      policy_entropy: 148.1654510498047
      policy_loss: 3.8087193965911865
      var_gnorm: 21.0423583984375
      vf_explained_var: 0.08520513772964478
      vf_loss: 2.6309642791748047
    num_steps_sampled: 80000
    num_steps_trained: 80000
    wait_time_ms: 233.189
  iterations_since_restore: 8
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 173.20708918571472
  time_this_iter_s: 23.537859678268433
  time_total_s: 173.20708918571472
  timestamp: 1594087910
  timesteps_since_restore: 80000
  timesteps_this_iter: 10000
  timesteps_total: 80000
  training_iteration: 8
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 13.5/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 173 s, 8 iter, 80000 ts, 903 rew

agent-1: 124.0
agent-2: 112.0
agent-3: 54.0
agent-4: 110.0
agent-5: 109.0
agent-6: 123.0
agent-7: 136.0
agent-8: 129.0
agent-9: 138.0
agent-10: 130.0
Sum Reward: 1165.0
Avg Reward: 116.5
Min Reward: 54.0
Max Reward: 138.0
Gini Coefficient: 0.09416309012875536
20:20 Ratio: 1.6809815950920246
Max-min Ratio: 2.5555555555555554
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-12-09
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 936.0
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 8
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.468
    dispatch_time_ms: 5.66
    learner:
      cur_lr: 0.0013546720147132874
      grad_gnorm: 39.99994659423828
      policy_entropy: 71.9474105834961
      policy_loss: 6.578001976013184
      var_gnorm: 21.236724853515625
      vf_explained_var: -0.1432356834411621
      vf_loss: 39.27589416503906
    num_steps_sampled: 90000
    num_steps_trained: 90000
    wait_time_ms: 203.289
  iterations_since_restore: 9
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 192.64618396759033
  time_this_iter_s: 19.43909478187561
  time_total_s: 192.64618396759033
  timestamp: 1594087929
  timesteps_since_restore: 90000
  timesteps_this_iter: 10000
  timesteps_total: 90000
  training_iteration: 9
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 14.0/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 192 s, 9 iter, 90000 ts, 936 rew

agent-1: 144.0
agent-2: 148.0
agent-3: 136.0
agent-4: 92.0
agent-5: 120.0
agent-6: 115.0
agent-7: 131.0
agent-8: 144.0
agent-9: 135.0
agent-10: 137.0
Sum Reward: 1302.0
Avg Reward: 130.2
Min Reward: 92.0
Max Reward: 148.0
Gini Coefficient: 0.06497695852534562
20:20 Ratio: 1.4106280193236715
Max-min Ratio: 1.608695652173913
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-12-33
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 976.6666666666666
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 9
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.482
    dispatch_time_ms: 7.593
    learner:
      cur_lr: 0.0013540060026571155
      grad_gnorm: 39.99999237060547
      policy_entropy: 132.6201629638672
      policy_loss: -28.98344612121582
      var_gnorm: 22.166351318359375
      vf_explained_var: -1.0
      vf_loss: 32.51400375366211
    num_steps_sampled: 100000
    num_steps_trained: 100000
    wait_time_ms: 231.222
  iterations_since_restore: 10
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 216.01692605018616
  time_this_iter_s: 23.370742082595825
  time_total_s: 216.01692605018616
  timestamp: 1594087953
  timesteps_since_restore: 100000
  timesteps_this_iter: 10000
  timesteps_total: 100000
  training_iteration: 10
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 14.6/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 216 s, 10 iter, 100000 ts, 977 rew

agent-1: 198.0
agent-2: 163.0
agent-3: 110.0
agent-4: 175.0
agent-5: 146.0
agent-6: 129.0
agent-7: 119.0
agent-8: 170.0
agent-9: 170.0
agent-10: 109.0
Sum Reward: 1489.0
Avg Reward: 148.9
Min Reward: 109.0
Max Reward: 198.0
Gini Coefficient: 0.11087978509066487
20:20 Ratio: 1.7031963470319635
Max-min Ratio: 1.81651376146789
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-12-53
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1027.9
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 10
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.467
    dispatch_time_ms: 5.587
    learner:
      cur_lr: 0.0013533399906009436
      grad_gnorm: 40.00002670288086
      policy_entropy: 113.18234252929688
      policy_loss: -89.73592376708984
      var_gnorm: 22.558263778686523
      vf_explained_var: 0.8611257076263428
      vf_loss: 226.0850067138672
    num_steps_sampled: 110000
    num_steps_trained: 110000
    wait_time_ms: 211.78
  iterations_since_restore: 11
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 236.05852150917053
  time_this_iter_s: 20.041595458984375
  time_total_s: 236.05852150917053
  timestamp: 1594087973
  timesteps_since_restore: 110000
  timesteps_this_iter: 10000
  timesteps_total: 110000
  training_iteration: 11
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 15.1/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 236 s, 11 iter, 110000 ts, 1.03e+03 rew

agent-1: 156.0
agent-2: 75.0
agent-3: 131.0
agent-4: 164.0
agent-5: 162.0
agent-6: 221.0
agent-7: 141.0
agent-8: 87.0
agent-9: 131.0
agent-10: 186.0
Sum Reward: 1454.0
Avg Reward: 145.4
Min Reward: 75.0
Max Reward: 221.0
Gini Coefficient: 0.15680880330123798
20:20 Ratio: 2.5123456790123457
Max-min Ratio: 2.9466666666666668
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-13-16
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1066.6363636363637
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 11
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 3.645
    dispatch_time_ms: 6.626
    learner:
      cur_lr: 0.0013526739785447717
      grad_gnorm: 39.999996185302734
      policy_entropy: 107.9083480834961
      policy_loss: 76.24433135986328
      var_gnorm: 23.080049514770508
      vf_explained_var: -0.5651580095291138
      vf_loss: 102.32396697998047
    num_steps_sampled: 120000
    num_steps_trained: 120000
    wait_time_ms: 209.405
  iterations_since_restore: 12
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 258.7062301635742
  time_this_iter_s: 22.647708654403687
  time_total_s: 258.7062301635742
  timestamp: 1594087996
  timesteps_since_restore: 120000
  timesteps_this_iter: 10000
  timesteps_total: 120000
  training_iteration: 12
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 15.7/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 258 s, 12 iter, 120000 ts, 1.07e+03 rew

agent-1: 119.0
agent-2: 123.0
agent-3: 97.0
agent-4: 89.0
agent-5: 125.0
agent-6: 133.0
agent-7: 151.0
agent-8: 154.0
agent-9: 167.0
agent-10: 139.0
Sum Reward: 1297.0
Avg Reward: 129.7
Min Reward: 89.0
Max Reward: 167.0
Gini Coefficient: 0.1015420200462606
20:20 Ratio: 1.7258064516129032
Max-min Ratio: 1.8764044943820224
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-13-36
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1085.8333333333333
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 12
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.463
    dispatch_time_ms: 7.223
    learner:
      cur_lr: 0.0013520079664885998
      grad_gnorm: 39.99999237060547
      policy_entropy: 98.0406723022461
      policy_loss: -27.979164123535156
      var_gnorm: 23.285320281982422
      vf_explained_var: 0.8430248498916626
      vf_loss: 68.87567901611328
    num_steps_sampled: 130000
    num_steps_trained: 130000
    wait_time_ms: 215.95
  iterations_since_restore: 13
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 279.28859210014343
  time_this_iter_s: 20.582361936569214
  time_total_s: 279.28859210014343
  timestamp: 1594088016
  timesteps_since_restore: 130000
  timesteps_this_iter: 10000
  timesteps_total: 130000
  training_iteration: 13
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 16.2/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 279 s, 13 iter, 130000 ts, 1.09e+03 rew

agent-1: 84.0
agent-2: 195.0
agent-3: 116.0
agent-4: 109.0
agent-5: 181.0
agent-6: 106.0
agent-7: 61.0
agent-8: 47.0
agent-9: 88.0
agent-10: 99.0
Sum Reward: 1086.0
Avg Reward: 108.6
Min Reward: 47.0
Max Reward: 195.0
Gini Coefficient: 0.22117863720073666
20:20 Ratio: 3.4814814814814814
Max-min Ratio: 4.148936170212766
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-13-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1085.8461538461538
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 13
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.598
    dispatch_time_ms: 7.011
    learner:
      cur_lr: 0.0013513419544324279
      grad_gnorm: 39.999996185302734
      policy_entropy: 111.20513153076172
      policy_loss: 16.416126251220703
      var_gnorm: 23.5073184967041
      vf_explained_var: 0.5158184766769409
      vf_loss: 26.621551513671875
    num_steps_sampled: 140000
    num_steps_trained: 140000
    wait_time_ms: 189.844
  iterations_since_restore: 14
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 302.04857063293457
  time_this_iter_s: 22.759978532791138
  time_total_s: 302.04857063293457
  timestamp: 1594088039
  timesteps_since_restore: 140000
  timesteps_this_iter: 10000
  timesteps_total: 140000
  training_iteration: 14
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 16.8/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 302 s, 14 iter, 140000 ts, 1.09e+03 rew

agent-1: 101.0
agent-2: 104.0
agent-3: 80.0
agent-4: 133.0
agent-5: 132.0
agent-6: 145.0
agent-7: 98.0
agent-8: 94.0
agent-9: 57.0
agent-10: 86.0
Sum Reward: 1030.0
Avg Reward: 103.0
Min Reward: 57.0
Max Reward: 145.0
Gini Coefficient: 0.13844660194174757
20:20 Ratio: 2.0291970802919708
Max-min Ratio: 2.543859649122807
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-14-20
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1081.857142857143
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 14
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.668
    dispatch_time_ms: 5.954
    learner:
      cur_lr: 0.001350675942376256
      grad_gnorm: 40.0
      policy_entropy: 77.813232421875
      policy_loss: -61.18265914916992
      var_gnorm: 23.676851272583008
      vf_explained_var: -0.8594735860824585
      vf_loss: 120.23219299316406
    num_steps_sampled: 150000
    num_steps_trained: 150000
    wait_time_ms: 228.842
  iterations_since_restore: 15
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 322.9893596172333
  time_this_iter_s: 20.940788984298706
  time_total_s: 322.9893596172333
  timestamp: 1594088060
  timesteps_since_restore: 150000
  timesteps_this_iter: 10000
  timesteps_total: 150000
  training_iteration: 15
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 17.3/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 322 s, 15 iter, 150000 ts, 1.08e+03 rew

agent-1: 47.0
agent-2: 108.0
agent-3: 110.0
agent-4: 92.0
agent-5: 94.0
agent-6: 121.0
agent-7: 105.0
agent-8: 89.0
agent-9: 100.0
agent-10: 117.0
Sum Reward: 983.0
Avg Reward: 98.3
Min Reward: 47.0
Max Reward: 121.0
Gini Coefficient: 0.10162767039674465
20:20 Ratio: 1.75
Max-min Ratio: 2.574468085106383
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-14-44
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1075.2666666666667
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 15
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.515
    dispatch_time_ms: 7.395
    learner:
      cur_lr: 0.001350010046735406
      grad_gnorm: 40.00002670288086
      policy_entropy: 65.58602905273438
      policy_loss: 54.060546875
      var_gnorm: 23.878759384155273
      vf_explained_var: 0.17780083417892456
      vf_loss: 133.01638793945312
    num_steps_sampled: 160000
    num_steps_trained: 160000
    wait_time_ms: 200.425
  iterations_since_restore: 16
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 346.9269986152649
  time_this_iter_s: 23.937638998031616
  time_total_s: 346.9269986152649
  timestamp: 1594088084
  timesteps_since_restore: 160000
  timesteps_this_iter: 10000
  timesteps_total: 160000
  training_iteration: 16
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 17.9/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 346 s, 16 iter, 160000 ts, 1.08e+03 rew

agent-1: 117.0
agent-2: 88.0
agent-3: 75.0
agent-4: 100.0
agent-5: 63.0
agent-6: 21.0
agent-7: 114.0
agent-8: 95.0
agent-9: 95.0
agent-10: 104.0
Sum Reward: 872.0
Avg Reward: 87.2
Min Reward: 21.0
Max Reward: 117.0
Gini Coefficient: 0.16077981651376147
20:20 Ratio: 2.75
Max-min Ratio: 5.571428571428571
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-15-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1062.5625
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 16
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.509
    dispatch_time_ms: 7.049
    learner:
      cur_lr: 0.001349344034679234
      grad_gnorm: 39.999996185302734
      policy_entropy: 97.53681182861328
      policy_loss: -20.296329498291016
      var_gnorm: 24.135997772216797
      vf_explained_var: 0.6733183860778809
      vf_loss: 353.7943115234375
    num_steps_sampled: 170000
    num_steps_trained: 170000
    wait_time_ms: 232.524
  iterations_since_restore: 17
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 368.18659567832947
  time_this_iter_s: 21.259597063064575
  time_total_s: 368.18659567832947
  timestamp: 1594088105
  timesteps_since_restore: 170000
  timesteps_this_iter: 10000
  timesteps_total: 170000
  training_iteration: 17
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 18.5/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 368 s, 17 iter, 170000 ts, 1.06e+03 rew

agent-1: 63.0
agent-2: 79.0
agent-3: 65.0
agent-4: 75.0
agent-5: 97.0
agent-6: 86.0
agent-7: 83.0
agent-8: 91.0
agent-9: 99.0
agent-10: 90.0
Sum Reward: 828.0
Avg Reward: 82.8
Min Reward: 63.0
Max Reward: 99.0
Gini Coefficient: 0.08019323671497584
20:20 Ratio: 1.53125
Max-min Ratio: 1.5714285714285714
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-15-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1048.764705882353
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 17
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.528
    dispatch_time_ms: 5.85
    learner:
      cur_lr: 0.0013486780226230621
      grad_gnorm: 40.0
      policy_entropy: 104.21185302734375
      policy_loss: -5.424942493438721
      var_gnorm: 24.288843154907227
      vf_explained_var: -1.0
      vf_loss: 3.8275184631347656
    num_steps_sampled: 180000
    num_steps_trained: 180000
    wait_time_ms: 195.327
  iterations_since_restore: 18
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 392.0635013580322
  time_this_iter_s: 23.87690567970276
  time_total_s: 392.0635013580322
  timestamp: 1594088129
  timesteps_since_restore: 180000
  timesteps_this_iter: 10000
  timesteps_total: 180000
  training_iteration: 18
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 19.1/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 392 s, 18 iter, 180000 ts, 1.05e+03 rew

agent-1: 98.0
agent-2: 64.0
agent-3: 111.0
agent-4: 107.0
agent-5: 111.0
agent-6: 98.0
agent-7: 115.0
agent-8: 145.0
agent-9: 106.0
agent-10: 123.0
Sum Reward: 1078.0
Avg Reward: 107.8
Min Reward: 64.0
Max Reward: 145.0
Gini Coefficient: 0.09350649350649351
20:20 Ratio: 1.654320987654321
Max-min Ratio: 2.265625
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-15-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1050.388888888889
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 18
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.513
    dispatch_time_ms: 6.663
    learner:
      cur_lr: 0.0013480120105668902
      grad_gnorm: 40.000003814697266
      policy_entropy: 107.83457946777344
      policy_loss: 50.68169021606445
      var_gnorm: 24.470718383789062
      vf_explained_var: 0.7199754118919373
      vf_loss: 103.37287902832031
    num_steps_sampled: 190000
    num_steps_trained: 190000
    wait_time_ms: 228.215
  iterations_since_restore: 19
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 412.5915279388428
  time_this_iter_s: 20.528026580810547
  time_total_s: 412.5915279388428
  timestamp: 1594088150
  timesteps_since_restore: 190000
  timesteps_this_iter: 10000
  timesteps_total: 190000
  training_iteration: 19
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 19.6/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 412 s, 19 iter, 190000 ts, 1.05e+03 rew

agent-1: 90.0
agent-2: 63.0
agent-3: 39.0
agent-4: 88.0
agent-5: 90.0
agent-6: 119.0
agent-7: 100.0
agent-8: 115.0
agent-9: 125.0
agent-10: 71.0
Sum Reward: 900.0
Avg Reward: 90.0
Min Reward: 39.0
Max Reward: 125.0
Gini Coefficient: 0.158
20:20 Ratio: 2.392156862745098
Max-min Ratio: 3.2051282051282053
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-16-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1042.4736842105262
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 19
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.667
    dispatch_time_ms: 7.51
    learner:
      cur_lr: 0.0013473459985107183
      grad_gnorm: 39.99999237060547
      policy_entropy: 105.91032409667969
      policy_loss: -5.022404193878174
      var_gnorm: 24.616779327392578
      vf_explained_var: 0.46763861179351807
      vf_loss: 4.6812052726745605
    num_steps_sampled: 200000
    num_steps_trained: 200000
    wait_time_ms: 196.776
  iterations_since_restore: 20
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 436.50011706352234
  time_this_iter_s: 23.908589124679565
  time_total_s: 436.50011706352234
  timestamp: 1594088174
  timesteps_since_restore: 200000
  timesteps_this_iter: 10000
  timesteps_total: 200000
  training_iteration: 20
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 20.3/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 436 s, 20 iter, 200000 ts, 1.04e+03 rew

agent-1: 138.0
agent-2: 121.0
agent-3: 120.0
agent-4: 100.0
agent-5: 147.0
agent-6: 119.0
agent-7: 138.0
agent-8: 116.0
agent-9: 124.0
agent-10: 93.0
Sum Reward: 1216.0
Avg Reward: 121.6
Min Reward: 93.0
Max Reward: 147.0
Gini Coefficient: 0.07220394736842105
20:20 Ratio: 1.4766839378238341
Max-min Ratio: 1.5806451612903225
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-16-35
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1051.15
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 20
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.51
    dispatch_time_ms: 5.855
    learner:
      cur_lr: 0.0013466799864545465
      grad_gnorm: 40.000003814697266
      policy_entropy: 82.9360122680664
      policy_loss: -19.710294723510742
      var_gnorm: 24.81633186340332
      vf_explained_var: 0.985884428024292
      vf_loss: 27.094024658203125
    num_steps_sampled: 210000
    num_steps_trained: 210000
    wait_time_ms: 214.06
  iterations_since_restore: 21
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 457.42533779144287
  time_this_iter_s: 20.925220727920532
  time_total_s: 457.42533779144287
  timestamp: 1594088195
  timesteps_since_restore: 210000
  timesteps_this_iter: 10000
  timesteps_total: 210000
  training_iteration: 21
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 20.8/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 457 s, 21 iter, 210000 ts, 1.05e+03 rew

agent-1: 115.0
agent-2: 94.0
agent-3: 112.0
agent-4: 117.0
agent-5: 94.0
agent-6: 106.0
agent-7: 97.0
agent-8: 124.0
agent-9: 93.0
agent-10: 93.0
Sum Reward: 1045.0
Avg Reward: 104.5
Min Reward: 93.0
Max Reward: 124.0
Gini Coefficient: 0.05885167464114833
20:20 Ratio: 1.2956989247311828
Max-min Ratio: 1.3333333333333333
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-16-58
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1050.857142857143
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 21
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.509
    dispatch_time_ms: 6.022
    learner:
      cur_lr: 0.0013460139743983746
      grad_gnorm: 40.00000762939453
      policy_entropy: 104.82730865478516
      policy_loss: 3.916815757751465
      var_gnorm: 25.0073184967041
      vf_explained_var: 0.9537656307220459
      vf_loss: 3.971824884414673
    num_steps_sampled: 220000
    num_steps_trained: 220000
    wait_time_ms: 185.657
  iterations_since_restore: 22
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 480.87852573394775
  time_this_iter_s: 23.453187942504883
  time_total_s: 480.87852573394775
  timestamp: 1594088218
  timesteps_since_restore: 220000
  timesteps_this_iter: 10000
  timesteps_total: 220000
  training_iteration: 22
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 21.4/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 480 s, 22 iter, 220000 ts, 1.05e+03 rew

agent-1: 116.0
agent-2: 68.0
agent-3: 75.0
agent-4: 182.0
agent-5: 139.0
agent-6: 103.0
agent-7: 64.0
agent-8: 123.0
agent-9: 77.0
agent-10: 76.0
Sum Reward: 1023.0
Avg Reward: 102.3
Min Reward: 64.0
Max Reward: 182.0
Gini Coefficient: 0.19012707722385142
20:20 Ratio: 2.4318181818181817
Max-min Ratio: 2.84375
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-17-19
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1049.590909090909
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 22
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.551
    dispatch_time_ms: 7.292
    learner:
      cur_lr: 0.0013453479623422027
      grad_gnorm: 40.0
      policy_entropy: 95.04232788085938
      policy_loss: -11.874917984008789
      var_gnorm: 25.12012481689453
      vf_explained_var: -0.7963423728942871
      vf_loss: 17.104524612426758
    num_steps_sampled: 230000
    num_steps_trained: 230000
    wait_time_ms: 231.604
  iterations_since_restore: 23
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 501.87227725982666
  time_this_iter_s: 20.993751525878906
  time_total_s: 501.87227725982666
  timestamp: 1594088239
  timesteps_since_restore: 230000
  timesteps_this_iter: 10000
  timesteps_total: 230000
  training_iteration: 23
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 21.9/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 501 s, 23 iter, 230000 ts, 1.05e+03 rew

agent-1: 98.0
agent-2: 104.0
agent-3: 91.0
agent-4: 102.0
agent-5: 105.0
agent-6: 126.0
agent-7: 66.0
agent-8: 106.0
agent-9: 108.0
agent-10: 106.0
Sum Reward: 1012.0
Avg Reward: 101.2
Min Reward: 66.0
Max Reward: 126.0
Gini Coefficient: 0.07035573122529644
20:20 Ratio: 1.4904458598726114
Max-min Ratio: 1.9090909090909092
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-17-43
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1047.9565217391305
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 23
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.736
    dispatch_time_ms: 8.488
    learner:
      cur_lr: 0.0013446819502860308
      grad_gnorm: 10.178884506225586
      policy_entropy: 77.52050018310547
      policy_loss: 3.7361254692077637
      var_gnorm: 25.20689582824707
      vf_explained_var: 0.09120041131973267
      vf_loss: 0.06924808770418167
    num_steps_sampled: 240000
    num_steps_trained: 240000
    wait_time_ms: 190.581
  iterations_since_restore: 24
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 525.9455509185791
  time_this_iter_s: 24.07327365875244
  time_total_s: 525.9455509185791
  timestamp: 1594088263
  timesteps_since_restore: 240000
  timesteps_this_iter: 10000
  timesteps_total: 240000
  training_iteration: 24
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 22.5/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 525 s, 24 iter, 240000 ts, 1.05e+03 rew

agent-1: 95.0
agent-2: 92.0
agent-3: 116.0
agent-4: 102.0
agent-5: 102.0
agent-6: 96.0
agent-7: 102.0
agent-8: 97.0
agent-9: 100.0
agent-10: 99.0
Sum Reward: 1001.0
Avg Reward: 100.1
Min Reward: 92.0
Max Reward: 116.0
Gini Coefficient: 0.03106893106893107
20:20 Ratio: 1.1657754010695187
Max-min Ratio: 1.2608695652173914
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-18-05
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1046.0
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 24
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.352
    dispatch_time_ms: 5.533
    learner:
      cur_lr: 0.0013440160546451807
      grad_gnorm: 25.56447982788086
      policy_entropy: 80.1393051147461
      policy_loss: -5.971991539001465
      var_gnorm: 25.406160354614258
      vf_explained_var: 0.9721763730049133
      vf_loss: 0.4996202886104584
    num_steps_sampled: 250000
    num_steps_trained: 250000
    wait_time_ms: 235.35
  iterations_since_restore: 25
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 547.7455501556396
  time_this_iter_s: 21.799999237060547
  time_total_s: 547.7455501556396
  timestamp: 1594088285
  timesteps_since_restore: 250000
  timesteps_this_iter: 10000
  timesteps_total: 250000
  training_iteration: 25
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 23.1/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 547 s, 25 iter, 250000 ts, 1.05e+03 rew

agent-1: 84.0
agent-2: 94.0
agent-3: 74.0
agent-4: 85.0
agent-5: 88.0
agent-6: 88.0
agent-7: 74.0
agent-8: 84.0
agent-9: 103.0
agent-10: 82.0
Sum Reward: 856.0
Avg Reward: 85.6
Min Reward: 74.0
Max Reward: 103.0
Gini Coefficient: 0.05186915887850467
20:20 Ratio: 1.3310810810810811
Max-min Ratio: 1.3918918918918919
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-18-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1038.4
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 25
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 3.052
    dispatch_time_ms: 6.364
    learner:
      cur_lr: 0.0013433500425890088
      grad_gnorm: 40.000003814697266
      policy_entropy: 120.3762435913086
      policy_loss: 7.107192516326904
      var_gnorm: 25.491350173950195
      vf_explained_var: 0.7909168004989624
      vf_loss: 6.146656036376953
    num_steps_sampled: 260000
    num_steps_trained: 260000
    wait_time_ms: 197.501
  iterations_since_restore: 26
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 571.6900532245636
  time_this_iter_s: 23.94450306892395
  time_total_s: 571.6900532245636
  timestamp: 1594088309
  timesteps_since_restore: 260000
  timesteps_this_iter: 10000
  timesteps_total: 260000
  training_iteration: 26
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 23.6/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 571 s, 26 iter, 260000 ts, 1.04e+03 rew

agent-1: 106.0
agent-2: 103.0
agent-3: 107.0
agent-4: 101.0
agent-5: 105.0
agent-6: 109.0
agent-7: 88.0
agent-8: 132.0
agent-9: 123.0
agent-10: 89.0
Sum Reward: 1063.0
Avg Reward: 106.3
Min Reward: 88.0
Max Reward: 132.0
Gini Coefficient: 0.06462841015992474
20:20 Ratio: 1.4406779661016949
Max-min Ratio: 1.5
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-18-50
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1039.3461538461538
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 26
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 3.896
    dispatch_time_ms: 6.24
    learner:
      cur_lr: 0.001342684030532837
      grad_gnorm: 34.136444091796875
      policy_entropy: 87.47720336914062
      policy_loss: -8.984675407409668
      var_gnorm: 25.676576614379883
      vf_explained_var: 0.9578282237052917
      vf_loss: 1.051055908203125
    num_steps_sampled: 270000
    num_steps_trained: 270000
    wait_time_ms: 237.529
  iterations_since_restore: 27
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 592.8576972484589
  time_this_iter_s: 21.167644023895264
  time_total_s: 592.8576972484589
  timestamp: 1594088330
  timesteps_since_restore: 270000
  timesteps_this_iter: 10000
  timesteps_total: 270000
  training_iteration: 27
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 24.2/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 592 s, 27 iter, 270000 ts, 1.04e+03 rew

agent-1: 104.0
agent-2: 94.0
agent-3: 78.0
agent-4: 75.0
agent-5: 88.0
agent-6: 106.0
agent-7: 100.0
agent-8: 58.0
agent-9: 84.0
agent-10: 78.0
Sum Reward: 865.0
Avg Reward: 86.5
Min Reward: 58.0
Max Reward: 106.0
Gini Coefficient: 0.09213872832369942
20:20 Ratio: 1.5789473684210527
Max-min Ratio: 1.8275862068965518
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-19-14
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1032.888888888889
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 27
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 3.664
    dispatch_time_ms: 8.315
    learner:
      cur_lr: 0.001342018018476665
      grad_gnorm: 20.114408493041992
      policy_entropy: 87.28755950927734
      policy_loss: -1.1214356422424316
      var_gnorm: 25.874269485473633
      vf_explained_var: 0.663442850112915
      vf_loss: 0.18534186482429504
    num_steps_sampled: 280000
    num_steps_trained: 280000
    wait_time_ms: 191.438
  iterations_since_restore: 28
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 616.6599655151367
  time_this_iter_s: 23.802268266677856
  time_total_s: 616.6599655151367
  timestamp: 1594088354
  timesteps_since_restore: 280000
  timesteps_this_iter: 10000
  timesteps_total: 280000
  training_iteration: 28
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 24.7/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 616 s, 28 iter, 280000 ts, 1.03e+03 rew

agent-1: 98.0
agent-2: 136.0
agent-3: 120.0
agent-4: 146.0
agent-5: 112.0
agent-6: 114.0
agent-7: 103.0
agent-8: 114.0
agent-9: 141.0
agent-10: 114.0
Sum Reward: 1198.0
Avg Reward: 119.8
Min Reward: 98.0
Max Reward: 146.0
Gini Coefficient: 0.06978297161936561
20:20 Ratio: 1.427860696517413
Max-min Ratio: 1.489795918367347
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-19-34
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1668.0
  episode_reward_mean: 1038.7857142857142
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 28
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.572
    dispatch_time_ms: 6.768
    learner:
      cur_lr: 0.0013413520064204931
      grad_gnorm: 40.0
      policy_entropy: 68.03922271728516
      policy_loss: 35.178890228271484
      var_gnorm: 26.076854705810547
      vf_explained_var: 0.23252081871032715
      vf_loss: 128.7083740234375
    num_steps_sampled: 290000
    num_steps_trained: 290000
    wait_time_ms: 166.413
  iterations_since_restore: 29
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 636.5146157741547
  time_this_iter_s: 19.854650259017944
  time_total_s: 636.5146157741547
  timestamp: 1594088374
  timesteps_since_restore: 290000
  timesteps_this_iter: 10000
  timesteps_total: 290000
  training_iteration: 29
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 25.3/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 636 s, 29 iter, 290000 ts, 1.04e+03 rew

agent-1: 168.0
agent-2: 191.0
agent-3: 197.0
agent-4: 184.0
agent-5: 162.0
agent-6: 185.0
agent-7: 166.0
agent-8: 197.0
agent-9: 178.0
agent-10: 156.0
Sum Reward: 1784.0
Avg Reward: 178.4
Min Reward: 156.0
Max Reward: 197.0
Gini Coefficient: 0.04461883408071749
20:20 Ratio: 1.2389937106918238
Max-min Ratio: 1.2628205128205128
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-19-54
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1064.4827586206898
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 29
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.671
    dispatch_time_ms: 6.913
    learner:
      cur_lr: 0.0013406859943643212
      grad_gnorm: 40.0
      policy_entropy: 38.97496032714844
      policy_loss: 25.73476791381836
      var_gnorm: 26.28670310974121
      vf_explained_var: -0.010509967803955078
      vf_loss: 78.572509765625
    num_steps_sampled: 300000
    num_steps_trained: 300000
    wait_time_ms: 191.423
  iterations_since_restore: 30
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 656.9488146305084
  time_this_iter_s: 20.43419885635376
  time_total_s: 656.9488146305084
  timestamp: 1594088394
  timesteps_since_restore: 300000
  timesteps_this_iter: 10000
  timesteps_total: 300000
  training_iteration: 30
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 25.8/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 656 s, 30 iter, 300000 ts, 1.06e+03 rew

agent-1: 118.0
agent-2: 144.0
agent-3: 118.0
agent-4: 90.0
agent-5: 104.0
agent-6: 126.0
agent-7: 120.0
agent-8: 133.0
agent-9: 104.0
agent-10: 146.0
Sum Reward: 1203.0
Avg Reward: 120.3
Min Reward: 90.0
Max Reward: 146.0
Gini Coefficient: 0.07938487115544472
20:20 Ratio: 1.4948453608247423
Max-min Ratio: 1.6222222222222222
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-20-15
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1069.1
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 30
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.489
    dispatch_time_ms: 6.935
    learner:
      cur_lr: 0.0013400199823081493
      grad_gnorm: 40.000003814697266
      policy_entropy: 44.02031326293945
      policy_loss: -9.45591926574707
      var_gnorm: 26.465957641601562
      vf_explained_var: 0.8672362565994263
      vf_loss: 114.61105346679688
    num_steps_sampled: 310000
    num_steps_trained: 310000
    wait_time_ms: 217.713
  iterations_since_restore: 31
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 677.5841245651245
  time_this_iter_s: 20.63530993461609
  time_total_s: 677.5841245651245
  timestamp: 1594088415
  timesteps_since_restore: 310000
  timesteps_this_iter: 10000
  timesteps_total: 310000
  training_iteration: 31
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 26.3/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 677 s, 31 iter, 310000 ts, 1.07e+03 rew

agent-1: 146.0
agent-2: 135.0
agent-3: 154.0
agent-4: 105.0
agent-5: 148.0
agent-6: 133.0
agent-7: 143.0
agent-8: 141.0
agent-9: 128.0
agent-10: 149.0
Sum Reward: 1382.0
Avg Reward: 138.2
Min Reward: 105.0
Max Reward: 154.0
Gini Coefficient: 0.05050651230101302
20:20 Ratio: 1.3004291845493563
Max-min Ratio: 1.4666666666666666
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-20-38
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1079.1935483870968
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 31
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.487
    dispatch_time_ms: 6.208
    learner:
      cur_lr: 0.0013393539702519774
      grad_gnorm: 40.0000114440918
      policy_entropy: 118.6013412475586
      policy_loss: -7.790137767791748
      var_gnorm: 26.73008918762207
      vf_explained_var: 0.6042559146881104
      vf_loss: 6.3701066970825195
    num_steps_sampled: 320000
    num_steps_trained: 320000
    wait_time_ms: 186.005
  iterations_since_restore: 32
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 700.3433122634888
  time_this_iter_s: 22.759187698364258
  time_total_s: 700.3433122634888
  timestamp: 1594088438
  timesteps_since_restore: 320000
  timesteps_this_iter: 10000
  timesteps_total: 320000
  training_iteration: 32
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 26.9/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 700 s, 32 iter, 320000 ts, 1.08e+03 rew

agent-1: 137.0
agent-2: 130.0
agent-3: 128.0
agent-4: 125.0
agent-5: 139.0
agent-6: 144.0
agent-7: 131.0
agent-8: 150.0
agent-9: 127.0
agent-10: 127.0
Sum Reward: 1338.0
Avg Reward: 133.8
Min Reward: 125.0
Max Reward: 150.0
Gini Coefficient: 0.03228699551569507
20:20 Ratio: 1.1666666666666667
Max-min Ratio: 1.2
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-20-59
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1087.28125
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 32
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 3.623
    dispatch_time_ms: 6.199
    learner:
      cur_lr: 0.0013386879581958055
      grad_gnorm: 40.000003814697266
      policy_entropy: 67.9868392944336
      policy_loss: 10.511895179748535
      var_gnorm: 26.84033966064453
      vf_explained_var: 0.6166824698448181
      vf_loss: 524.1788330078125
    num_steps_sampled: 330000
    num_steps_trained: 330000
    wait_time_ms: 212.018
  iterations_since_restore: 33
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 721.8547832965851
  time_this_iter_s: 21.511471033096313
  time_total_s: 721.8547832965851
  timestamp: 1594088459
  timesteps_since_restore: 330000
  timesteps_this_iter: 10000
  timesteps_total: 330000
  training_iteration: 33
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 27.4/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 721 s, 33 iter, 330000 ts, 1.09e+03 rew

agent-1: 94.0
agent-2: 128.0
agent-3: 111.0
agent-4: 104.0
agent-5: 99.0
agent-6: 104.0
agent-7: 85.0
agent-8: 104.0
agent-9: 78.0
agent-10: 83.0
Sum Reward: 990.0
Avg Reward: 99.0
Min Reward: 78.0
Max Reward: 128.0
Gini Coefficient: 0.07838383838383839
20:20 Ratio: 1.484472049689441
Max-min Ratio: 1.641025641025641
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-21-23
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1084.3333333333333
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 33
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.437
    dispatch_time_ms: 6.234
    learner:
      cur_lr: 0.0013380219461396337
      grad_gnorm: 40.000003814697266
      policy_entropy: 37.36085510253906
      policy_loss: 9.183853149414062
      var_gnorm: 26.975242614746094
      vf_explained_var: -0.23841989040374756
      vf_loss: 162.061767578125
    num_steps_sampled: 340000
    num_steps_trained: 340000
    wait_time_ms: 203.11
  iterations_since_restore: 34
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 745.3165566921234
  time_this_iter_s: 23.46177339553833
  time_total_s: 745.3165566921234
  timestamp: 1594088483
  timesteps_since_restore: 340000
  timesteps_this_iter: 10000
  timesteps_total: 340000
  training_iteration: 34
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 27.9/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 745 s, 34 iter, 340000 ts, 1.08e+03 rew

agent-1: 109.0
agent-2: 132.0
agent-3: 111.0
agent-4: 110.0
agent-5: 95.0
agent-6: 117.0
agent-7: 138.0
agent-8: 128.0
agent-9: 90.0
agent-10: 144.0
Sum Reward: 1174.0
Avg Reward: 117.4
Min Reward: 90.0
Max Reward: 144.0
Gini Coefficient: 0.08194207836456559
20:20 Ratio: 1.5243243243243243
Max-min Ratio: 1.6
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-21-45
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1086.9705882352941
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 34
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.535
    dispatch_time_ms: 5.868
    learner:
      cur_lr: 0.0013373560504987836
      grad_gnorm: 5.981940269470215
      policy_entropy: 117.15750122070312
      policy_loss: 0.6664355993270874
      var_gnorm: 27.130868911743164
      vf_explained_var: 0.7612856030464172
      vf_loss: 0.5537713766098022
    num_steps_sampled: 350000
    num_steps_trained: 350000
    wait_time_ms: 230.152
  iterations_since_restore: 35
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 767.5250370502472
  time_this_iter_s: 22.20848035812378
  time_total_s: 767.5250370502472
  timestamp: 1594088505
  timesteps_since_restore: 350000
  timesteps_this_iter: 10000
  timesteps_total: 350000
  training_iteration: 35
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 28.5/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 767 s, 35 iter, 350000 ts, 1.09e+03 rew

agent-1: 84.0
agent-2: 85.0
agent-3: 80.0
agent-4: 83.0
agent-5: 60.0
agent-6: 87.0
agent-7: 98.0
agent-8: 80.0
agent-9: 101.0
agent-10: 85.0
Sum Reward: 843.0
Avg Reward: 84.3
Min Reward: 60.0
Max Reward: 101.0
Gini Coefficient: 0.06370106761565836
20:20 Ratio: 1.4214285714285715
Max-min Ratio: 1.6833333333333333
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-22-08
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1080.0
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 35
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.771
    dispatch_time_ms: 7.515
    learner:
      cur_lr: 0.0013366900384426117
      grad_gnorm: 40.00000762939453
      policy_entropy: 127.0151138305664
      policy_loss: 1.975196361541748
      var_gnorm: 27.22966766357422
      vf_explained_var: 0.854552686214447
      vf_loss: 15.842794418334961
    num_steps_sampled: 360000
    num_steps_trained: 360000
    wait_time_ms: 185.261
  iterations_since_restore: 36
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 790.4860053062439
  time_this_iter_s: 22.960968255996704
  time_total_s: 790.4860053062439
  timestamp: 1594088528
  timesteps_since_restore: 360000
  timesteps_this_iter: 10000
  timesteps_total: 360000
  training_iteration: 36
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 29.1/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 790 s, 36 iter, 360000 ts, 1.08e+03 rew

agent-1: 114.0
agent-2: 103.0
agent-3: 99.0
agent-4: 127.0
agent-5: 103.0
agent-6: 128.0
agent-7: 149.0
agent-8: 93.0
agent-9: 101.0
agent-10: 88.0
Sum Reward: 1105.0
Avg Reward: 110.5
Min Reward: 88.0
Max Reward: 149.0
Gini Coefficient: 0.08805429864253393
20:20 Ratio: 1.5303867403314917
Max-min Ratio: 1.6931818181818181
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-22-29
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1080.6944444444443
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 36
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 3.569
    dispatch_time_ms: 6.119
    learner:
      cur_lr: 0.0013360240263864398
      grad_gnorm: 40.000003814697266
      policy_entropy: 68.03662872314453
      policy_loss: -58.58308410644531
      var_gnorm: 27.363924026489258
      vf_explained_var: 0.8282730579376221
      vf_loss: 222.7348175048828
    num_steps_sampled: 370000
    num_steps_trained: 370000
    wait_time_ms: 216.603
  iterations_since_restore: 37
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 811.2947824001312
  time_this_iter_s: 20.80877709388733
  time_total_s: 811.2947824001312
  timestamp: 1594088549
  timesteps_since_restore: 370000
  timesteps_this_iter: 10000
  timesteps_total: 370000
  training_iteration: 37
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 29.6/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 811 s, 37 iter, 370000 ts, 1.08e+03 rew

agent-1: 90.0
agent-2: 124.0
agent-3: 81.0
agent-4: 113.0
agent-5: 108.0
agent-6: 134.0
agent-7: 140.0
agent-8: 122.0
agent-9: 124.0
agent-10: 122.0
Sum Reward: 1158.0
Avg Reward: 115.8
Min Reward: 81.0
Max Reward: 140.0
Gini Coefficient: 0.08221070811744387
20:20 Ratio: 1.6023391812865497
Max-min Ratio: 1.728395061728395
Result for A3C_harvest_env_0:
  custom_metrics: {}
  date: 2020-07-06_22-22-52
  done: false
  episode_len_mean: 1000.0
  episode_reward_max: 1784.0
  episode_reward_mean: 1082.7837837837837
  episode_reward_min: -574.0
  episodes_this_iter: 1
  episodes_total: 37
  experiment_id: e5456c3ed0ea4a149a845d45ac61f7d5
  hostname: gpu011
  info:
    apply_time_ms: 2.653
    dispatch_time_ms: 6.769
    learner:
      cur_lr: 0.001335358014330268
      grad_gnorm: 40.00000762939453
      policy_entropy: 122.2182388305664
      policy_loss: 5.035829544067383
      var_gnorm: 27.537626266479492
      vf_explained_var: 0.46741682291030884
      vf_loss: 3.196992874145508
    num_steps_sampled: 380000
    num_steps_trained: 380000
    wait_time_ms: 188.093
  iterations_since_restore: 38
  node_ip: 172.17.8.11
  num_metric_batches_dropped: 0
  pid: 13526
  policy_reward_mean: {}
  time_since_restore: 834.388866186142
  time_this_iter_s: 23.094083786010742
  time_total_s: 834.388866186142
  timestamp: 1594088572
  timesteps_since_restore: 380000
  timesteps_this_iter: 10000
  timesteps_total: 380000
  training_iteration: 38
  
== Status ==
Using FIFO scheduling algorithm.
Resources requested: 2.0/2 CPUs, 0/1 GPUs
Memory usage on this node: 30.2/202.5 GB
Result logdir: /h/zhaostep/ray_results/harvest_A3C
RUNNING trials:
 - A3C_harvest_env_0:	RUNNING [pid=13526], 834 s, 38 iter, 380000 ts, 1.08e+03 rew

agent-1: 120.0
agent-2: 120.0
agent-3: 82.0
agent-4: 119.0
agent-5: 112.0
agent-6: 95.0
agent-7: 115.0
agent-8: 140.0
agent-9: 124.0
agent-10: 107.0
Sum Reward: 1134.0
Avg Reward: 113.4
Min Reward: 82.0
Max Reward: 140.0
Gini Coefficient: 0.07213403880070547
20:20 Ratio: 1.4915254237288136
Max-min Ratio: 1.7073170731707317
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0706 22:23:13.757846 13566 raylet_client.cc:345] IOError: [RayletClient] Connection closed unexpectedly. [RayletClient] Failed to push profile events.
F0706 22:23:13.841013 13528 raylet_extension.cc:220]  Check failed: _s.ok() [RayletClient] Failed to push errors to raylet.: IOError: [RayletClient] Connection closed unexpectedly.
*** Check failure stack trace: ***
Fatal Python error: Aborted

Stack (most recent call first):
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/utils.py", line 73 in push_error_to_driver
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/workers/default_worker.py", line 114 in <module>
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0706 22:23:14.354717 13526 raylet_extension.cc:190]  Check failed: _s.ok() [RayletClient] Failed to wait for objects.: IOError: [RayletClient] Raylet connection closed.
*** Check failure stack trace: ***
Fatal Python error: Aborted

Stack (most recent call first):
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/worker.py", line 2485 in wait
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/rllib/optimizers/async_gradients_optimizer.py", line 46 in step
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/rllib/agents/a3c/a3c.py", line 68 in _train
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/tune/trainable.py", line 146 in train
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/rllib/agents/agent.py", line 279 in train
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/function_manager.py", line 713 in actor_method_executor
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/worker.py", line 820 in _process_task
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/worker.py", line 919 in _wait_for_and_process_task
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/worker.py", line 962 in main_loop
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/workers/default_worker.py", line 107 in <module>
WARNING: Logging before InitGoogleLogging() is written to STDERR
F0706 22:23:14.355998 13370 raylet_extension.cc:190]  Check failed: _s.ok() [RayletClient] Failed to wait for objects.: IOError: [RayletClient] Raylet connection closed.
*** Check failure stack trace: ***
Fatal Python error: Aborted

Stack (most recent call first):
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/worker.py", line 2485 in wait
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/tune/ray_trial_executor.py", line 199 in get_next_available_trial
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 259 in _process_events
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/tune/trial_runner.py", line 118 in step
  File "/h/zhaostep/.conda/envs/causal/lib/python3.6/site-packages/ray/tune/tune.py", line 108 in run_experiments
  File "train_agents.py", line 216 in main
  File "train_agents.py", line 238 in <module>
srun: error: gpu011: task 0: Aborted
